<!DOCTYPE html>
<html lang="en-AU" dir="ltr">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, viewport-fit=cover">
    <title>What Are We Even Doing?</title>

    <meta name="author" content="Shavon Nand">
    <meta name="robots" content="noai, noimageai">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,200..900;1,8..60,200..900&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/assets/css/styles.css" />

    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="/assets/favicon/site.webmanifest">

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>What Are We Even Doing?</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="What Are We Even Doing?" />
<meta name="author" content="Shavon Nand" />
<meta property="og:locale" content="en_AU" />
<meta name="description" content="Whilst browsing YouTube I came across AI In Context’s ‘We’re Not Ready for Superintelligence’. This video, to me, was conspiratorial and made a lot of assumptions that just felt wrong. I mean just look at the description of the video:" />
<meta property="og:description" content="Whilst browsing YouTube I came across AI In Context’s ‘We’re Not Ready for Superintelligence’. This video, to me, was conspiratorial and made a lot of assumptions that just felt wrong. I mean just look at the description of the video:" />
<link rel="canonical" href="http://localhost:4000/2025/10/08/what_are_we_even_doing.html" />
<meta property="og:url" content="http://localhost:4000/2025/10/08/what_are_we_even_doing.html" />
<meta property="og:image" content="http://localhost:4000/assets/images/what_are_we_even_doing/Screenshot%202025-10-08%20at%2018-05-46%20ChatGPT.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-08T00:00:00+10:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/images/what_are_we_even_doing/Screenshot%202025-10-08%20at%2018-05-46%20ChatGPT.png" />
<meta property="twitter:title" content="What Are We Even Doing?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Shavon Nand"},"dateModified":"2025-10-08T00:00:00+10:00","datePublished":"2025-10-08T00:00:00+10:00","description":"Whilst browsing YouTube I came across AI In Context’s ‘We’re Not Ready for Superintelligence’. This video, to me, was conspiratorial and made a lot of assumptions that just felt wrong. I mean just look at the description of the video:","headline":"What Are We Even Doing?","image":"http://localhost:4000/assets/images/what_are_we_even_doing/Screenshot%202025-10-08%20at%2018-05-46%20ChatGPT.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/10/08/what_are_we_even_doing.html"},"url":"http://localhost:4000/2025/10/08/what_are_we_even_doing.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <div class="page-container">
      

<nav role="navigation" data-nosnippet="" aria-label="Main">
  <!-- Logo block -->
  <div class="logo-image">
    <a class="u-url u-uid" href="/">
      <picture>
        <source media="(min-width: 769px)" srcset="/assets/images/shavon.webp" type="image/webp">
        <source media="(max-width: 768px)" srcset="/assets/images/shavon_horizontal.webp" type="image/webp">
        <img class="u-photo u-logo" src="/assets/images/shavon.jpeg" type="image/webp" alt="A photograph of Shavon Nand as a child sitting in a child safety seat.">
      </picture>
    </a>
  </div>

  <!-- Menu toggle -->
  <button class="menu-toggle" id="menu-toggle">Menu</button>

  <!-- Navigation links -->
  <div class="nav-links collapsed" id="nav-links">
    <ul class="top-level">
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/writing.html">Writing</a></li>
      
        <li><a href="/blogroll.html">Blogroll</a></li>
      
      <li><a href="/search/" class="button">Search</a></li>
    </ul>

    <ul>
      <li class="nav-header">Social</li>
      <li><a class="u-url" href="https://www.linkedin.com/in/shavon-nand-a77185245/" rel="me">LinkedIn</a></li>
      <li><a class="u-url" href="https://letterboxd.com/SCN/" rel="me">Letterboxd</a></li>
      <li><a class="u-url" href="https://github.com/snand5" rel="me">GitHub</a></li>
      <li><a class="u-url" href="https://www.youtube.com/@shavon" rel="me">YouTube</a></li>
      <li><a class="u-url" href="/feed.xml">RSS</a></li>
      
    </ul>
  </div>
</nav>

<script src="/assets/js/nav.js"></script>

      <main id="main" tabindex="-1" role="main">
        <article class="post">
  <header class="post-header" role="region" aria-label="Post header">
    <h1 class="post-title">What Are We Even Doing?</h1>
    
    <h3 class="subtitle">or: How I Learned to Stop Worrying and Love Artificial Intelligence</h3>
    
    <div class="byline">
      <a href="/writing.html">
        <div class="avatar">
          <img src="/assets/images/avatar.jpeg" alt="Shavon Nand avatar">
        </div>
      </a>
      <div class="author-meta">
        <a href="/writing.html" style="text-decoration: none;"><span
            class="author-name">Shavon Nand</span></a>
        <time class="post-date" datetime="2025-10-08T00:00:00+10:00">
          8 October, 2025
        </time>
      </div>
    </div>
  </header><figure class="in-text-image bordered">
    <img src="/assets/images/what_are_we_even_doing/Screenshot%202025-10-08%20at%2018-05-46%20ChatGPT.png"
      width="1456" height="819"
      alt="Screenshot of OpenAI's ChatGPT website on Desktop."
      loading="lazy"
      class="clickable-image sizing-normal">
    <div class="view-image-btn">
      <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20"
        viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2"
        stroke-linecap="round" stroke-linejoin="round"
        class="lucide lucide-maximize2 lucide-maximize-2">
        <polyline points="15 3 21 3 21 9"></polyline>
        <polyline points="9 21 3 21 3 15"></polyline>
        <line x1="21" x2="14" y1="3" y2="10"></line>
        <line x1="3" x2="10" y1="21" y2="14"></line>
      </svg>
    </div>
    
    <figcaption>Screenshot of OpenAI's ChatGPT website on Desktop.</figcaption>
    
  </figure><div class="available-content">
    <div class="article-content">
      <p>Whilst browsing YouTube I came across <a href="https://www.youtube.com/@AI_In_Context">AI In Context</a>’s ‘<a href="https://www.youtube.com/watch?v=5KVDDfAkRgc">We’re Not Ready for Superintelligence</a>’. This video, to me, was conspiratorial and made a lot of assumptions that just felt wrong. I mean just look at the description of the video:</p>

<blockquote>
  <p>AI 2027 depicts a possible future where artificial intelligence radically transforms the world in just a few intense years. It’s based on detailed expert forecasts — but how much of it will actually happen? Are we really racing towards a choice between a planet controlled by the elite, or one where humans have lost control entirely?</p>

  <p>My takeaway? Loss of control, racing scenarios, and concentration of power are all concerningly plausible, and among the most pressing issues the world faces. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>
</blockquote>

<iframe src="https://www.youtube.com/embed/5KVDDfAkRgc" title="We&#39;re Not Ready for Superintelligence" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>So after watching that conspiratorial apocalyptic mess, I decided to do some digging, and this is what I found.</p>

<p><a href="https://www.imdb.com/name/nm5354308/">Aric Floyd</a>, the host of the channel is an actor. The channel is not the a personal project of of some AI enthusiast. It was created by the nonprofit ‘<a href="https://80000hours.org/">80,000 Hours</a>’.</p>

<p>You might recognize 80,000 Hours from a YouTube sponsor read. Here’s one I found from Extra Credits:</p>

<blockquote>
  <p>80,000 Hours is a nonprofit organization dedicated to helping people find careers that are not only fulfilling but also have a significant positive impact on the world. Their work is based on over 10 years of research, conducted in collaboration with Oxford University, aimed at identifying high-impact career paths that can tackle some of humanity’s biggest challenges.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>
</blockquote>

<p>So you might be wondering as to why a career training website would be producing an apocalyptic AI video about how humanity will need to fight against an all powerful AI. The answer is that 80,000 Hours aligns itself with the philosophy of effective altruism.</p>

<blockquote>
  <p>Effective Altruism is a form of utilitarianism, itself a branch of consequentialism – the plausible idea that you judge choices by their consequences. Utilitarians go further by saying you judge acts only by their consequences, and that the outcomes can be graded on a single scale of utility, or happiness. You judge acts by comparing their effects on the global sum of happiness, and the morally right course is the one you expect to achieve the maximum effect. Crucially, other moral considerations – obligations to be honest, to be just, to be loyal, to respect property rights and many more – count only to the extent that they bear on the happiness calculation. <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">3</a></sup></p>
</blockquote>

<p>I had first heard of effective altruism in 2022, as the philosophy of Sam Bankman-Fried. When FTX collapsed and Sam Bankman-Fried was sentenced on seven counts of wire fraud and conspiracy to launder money<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">4</a></sup>, I thought that would be the end for this movement I was wrong.</p>

<blockquote>
  <p>The philosophy of effective altruism makes the case that one way that idealistic young people with fancy degrees could improve the world is not by working at nonprofits, but by earning as much money as possible through finance, and then donating it all to vetted causes. <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">5</a></sup></p>
</blockquote>

<p>80,000 Hours advocates for a specific kind of effective altruism called “longtermism”<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>, the idea that “…because so many more humans and other intelligent beings could live in the future than live today, the most important thing for altruistic people to do in the present is to promote the welfare of those unborn beings, by ensuring that future comes to be by preventing existential risks — and that such a future is as good as possible.”<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup></p>

<blockquote>
  <p>For various reasons, the [Effective Altruism] movement has turned its attention toward longtermism—a more radical form of its utilitarianism that weighs the value of each future potential life approximately the same as a living person’s. Because any human extinction event, however unlikely, imposes infinite costs, longtermists can place enormous moral value on reducing whatever they view as existential risk. <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">8</a></sup></p>
</blockquote>

<blockquote>
  <p>[Effective Altruists] initially focused mostly on issues like animal welfare and global poverty, but over time, worries about an AI-fueled apocalypse became a central focus. With funding from deep-pocketed donors like billionaire and Facebook co-founder Dustin Moskovitz, they built their own insular universe to study AI safety, including a web of nonprofits and research organizations, forecasting centers, conferences, and web forums.<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup></p>
</blockquote>

<blockquote>
  <p>The most ardent advocates of effective altruism, or EA, believe researchers are only months or years away from building an AI superintelligence able to outsmart the world’s collective efforts to control it. Through either its own volition or via terrorists seeking to develop deadly bioweapons, such an AI could wipe out humanity, they say. And some, including noted EA thinker Eliezer Yudkowsky, believe even a nuclear holocaust would be preferable to an unchecked AI future. <sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup></p>
</blockquote>

<blockquote>
  <p>Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that’s what it takes to reduce the risk of large AI training runs. <sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup></p>
</blockquote>

<p>This to me does not sound like philosophy, this sounds like an AI <a href="https://en.wikipedia.org/wiki/Doomsday_cult">doomsday cult</a>.</p>

<blockquote>
  <p>[Effective Altruism]’s career advice center, 80,000 hours, lists “AI safety technical research” and “shaping future governance of AI” as the top two recommended careers for EAs to go into, and the billionaire EA class funds initiatives attempting to stop an AGI apocalypse. According to EAs, AGI is likely inevitable, and their goal is thus to make it beneficial to humanity: akin to creating a benevolent god rather than a devil. <sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup></p>
</blockquote>

<p>To borrow a term from Lee Vinsel, this AI apocalypse is another form of criti-hype.<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">13</a></sup> Basically another <a href="https://www.scientificamerican.com/article/premature-freak-outs-about-techno-enhancement/">Premature Freak-Out about Technological Enhancement</a>.</p>

<blockquote>
  <p>Perhaps the two most striking examples of this criti-hype trend are Shoshana Zuboff’s book, The Age of Surveillance Capitalism, and the film The Social Dilemma, which includes Zuboff and another criti-hyper Tristan Harris as talking heads. <sup id="fnref:15:1" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">13</a></sup></p>
</blockquote>

<p>Speaking of criti-hyper Tristan Harris<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">14</a></sup>, I came across this interview he did with Jon Stewart on the Daily Show.</p>

<iframe src="https://www.youtube.com/embed/675d_6WGPbo" title="Tristan Harris – The Dangers of Unregulated AI on Humanity &amp; the Workforce | The Daily Show" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Tristan Harris, after peddling the idea that “social media companies to puppet masters who have users on strings” <sup id="fnref:15:2" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">13</a></sup> has now moved onto his next target of AI. Falling into the same industry propaganda that he did last time.</p>

<p>This is not to say that there aren’t any real reasons to be critical of AI. Back in 2020, Robert Julian-Borchak Williams was “wrongfully arrested based on a flawed match from a facial recognition algorithm”. <sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">15</a></sup> That same year, “two professors and a graduate student had developed a facial-recognition program that could predict whether someone would be a criminal”.<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">16</a></sup></p>

<blockquote>
  <p>Advances in data science and machine learning have led to numerous algorithms in recent years that purport to predict crimes or criminality. But if the data used to build those algorithms is biased, the algorithms’ predictions will also be biased. Because of the racially skewed nature of policing in the US, the letter argues, any predictive algorithm modeling criminality will only reproduce the biases already reflected in the criminal justice system. <sup id="fnref:17:1" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">16</a></sup></p>
</blockquote>

<p>This is not even to mention the horrifying and harmful firehose of AI Slop that is now being generated en masse. <sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">17</a></sup><sup id="fnref:20" role="doc-noteref"><a href="#fn:20" class="footnote" rel="footnote">18</a></sup><sup id="fnref:21" role="doc-noteref"><a href="#fn:21" class="footnote" rel="footnote">19</a></sup><sup id="fnref:22" role="doc-noteref"><a href="#fn:22" class="footnote" rel="footnote">20</a></sup><sup id="fnref:23" role="doc-noteref"><a href="#fn:23" class="footnote" rel="footnote">21</a></sup><sup id="fnref:24" role="doc-noteref"><a href="#fn:24" class="footnote" rel="footnote">22</a></sup><sup id="fnref:25" role="doc-noteref"><a href="#fn:25" class="footnote" rel="footnote">23</a></sup><sup id="fnref:26" role="doc-noteref"><a href="#fn:26" class="footnote" rel="footnote">24</a></sup></p>

<p>I think that we need to treat AI as <a href="https://knightcolumbia.org/content/ai-as-normal-technology">Normal Technology</a>.</p>

<blockquote>
  <p>The statement “AI is normal technology” is three things: a description of current AI, a prediction about the foreseeable future of AI, and a prescription about how we should treat it. We view AI as a tool that we can and should remain in control of, and we argue that this goal does not require drastic policy interventions or technical breakthroughs. We do not think that viewing AI as a humanlike intelligence is currently accurate or useful for understanding its societal impacts, nor is it likely to be in our vision of the future. <sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">25</a></sup></p>
</blockquote>

<blockquote>
  <p>That some significant portion of OpenAI’s consumer base is using ChatGPT not so much for the expected “normal” uses like search, or productivity improvements, or creating slop birthday-party invitations, but for friendship, companionship, romance, and therapy certainly feels abnormal. (And apocalyptic.) But this is 2025, and intense, emotional, addiction-resembling attachment to software-bound experience has been a core paradigm of the technology industry for almost two decades, not to mention a multibillion-dollar business model. Certainly, you will not find me arguing that “psychosis-inducing sycophantic girlfriend robot subscription product” is “normal” in the sense of “acceptable” or “appropriate to a mature and dignified civilization.” But speaking descriptively, as a matter of long precedent, what could be more normal, in Silicon Valley, than people weeping on a message board because a UX change has transformed the valence of their addiction? <sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">26</a></sup></p>
</blockquote>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a></li>
  <li><a href="https://joinreboot.org/p/alignment">The Artificiality of Alignment</a></li>
  <li><a href="https://www.argmin.net/p/the-banal-evil-of-ai-safety">The Banal Evil of AI Safety</a></li>
  <li><a href="https://realizable.substack.com/p/artificial-intelligence-as-sorcery">Artificial Intelligence as Sorcery</a></li>
  <li><a href="https://aiguide.substack.com/p/magical-thinking-on-ai">Magical Thinking on AI</a></li>
  <li><a href="https://www.science.org/content/article/far-more-authors-use-ai-write-science-papers-admit-it-publisher-reports">Far more authors use AI to write science papers than admit it, publisher reports</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=5KVDDfAkRgc">We’re Not Ready for Superintelligence</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=9PYewjJKD-4">The Dev’s Creed: Being Wrong is Essential</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://www.bloomberg.com/opinion/articles/2023-10-18/effective-altruism-is-as-bankrupt-as-samuel-bankman-fried-s-ftx">Effective Altruism Is as Bankrupt as Sam Bankman-Fried’s FTX</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://www.bbc.com/worklife/article/20231009-ftxs-sam-bankman-fried-believed-in-effective-altruism-what-is-it">FTX’s Sam Bankman-Fried believed in ‘effective altruism’. What is it?</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://fortune.com/2024/09/02/effective-altruism-sam-bankman-frieds-bahamas-penthouse-ftx-crypto-finance/">Behind the scenes of how ‘effective altruism’ went to die in Sam Bankman-Fried’s Bahamas penthouse</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="https://80000hours.org/articles/future-generations/">Longtermism: a call to protect future generations</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p><a href="https://www.vox.com/future-perfect/23500014/effective-altruism-sam-bankman-fried-ftx-crypto">How effective altruism let Sam Bankman-Fried happen</a> <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p><a href="https://reason.com/2024/07/05/the-authoritarian-side-of-effective-altruism-comes-for-ai/">The Authoritarian Side of Effective Altruism Comes for AI</a> <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p><a href="https://www.semafor.com/article/11/21/2023/how-effective-altruism-led-to-a-crisis-at-openai">The AI industry turns against its favorite philosophy</a> <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p><a href="https://www.politico.com/news/2023/12/30/ai-debate-culture-clash-dc-silicon-valley-00133323">When Silicon Valley’s AI warriors came to Washington</a> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p><a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">Pausing AI Developments Isn’t Enough. We Need to Shut it All Down</a> <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p><a href="https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/">Effective Altruism Is Pushing a Dangerous Brand of ‘AI Safety’</a> <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p><a href="https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5">You’re Doing It Wrong: Notes on Criticism and Technology Hype</a> <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:15:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:15:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:18" role="doc-endnote">
      <p><a href="https://80000hours.org/podcast/episodes/tristan-harris-changing-incentives-social-media/">Tristan Harris on the need to change the incentives of social media companies</a> <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16" role="doc-endnote">
      <p><a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">Wrongfully Accused by an Algorithm</a> <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p><a href="https://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/">An Algorithm That ‘Predicts’ Criminality Based on a Face Sparks a Furor</a> <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:17:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p><a href="https://www.theguardian.com/technology/2024/mar/06/microsoft-ai-explicit-image-safety">Microsoft ignored safety problems with AI image generator, engineer complains</a> <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:20" role="doc-endnote">
      <p><a href="https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/">This is how AI image generators see the world</a> <a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:21" role="doc-endnote">
      <p><a href="https://www.404media.co/4chan-uses-bing-to-flood-the-internet-with-racist-images/">4chan Uses Bing to Flood the Internet With Racist Images</a> <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:22" role="doc-endnote">
      <p><a href="https://www.404media.co/where-facebooks-ai-slop-comes-from/">Where Facebook’s AI Slop Comes From</a> <a href="#fnref:22" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:23" role="doc-endnote">
      <p><a href="https://www.washingtonpost.com/technology/2023/12/14/ai-hate-memes-antisemitic-musk-x/">AI-generated Nazi memes thrive on Musk’s X despite claims of crackdown</a> <a href="#fnref:23" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:24" role="doc-endnote">
      <p><a href="https://www.404media.co/brainrot-ai-on-instagram-is-monetizing-the-most-fucked-up-things-you-can-imagine-and-lots-you-cant/">‘Brainrot’ AI on Instagram Is Monetizing the Most Fucked Up Things You Can Imagine (and Lots You Can’t)</a> <a href="#fnref:24" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:25" role="doc-endnote">
      <p><a href="https://www.mediamatters.org/tiktok/racist-ai-generated-videos-are-newest-slop-garnering-millions-views-tiktok">Racist AI-generated videos are the newest slop garnering millions of views on TikTok</a> <a href="#fnref:25" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:26" role="doc-endnote">
      <p><a href="https://www.theguardian.com/technology/2025/jan/13/just-the-start-xs-new-ai-software-driving-online-racist-abuse-experts-warn">‘Just the start’: X’s new AI software driving online racist abuse, experts warn</a> <a href="#fnref:26" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p><a href="https://knightcolumbia.org/content/ai-as-normal-technology">AI as Normal Technology</a> <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p><a href="https://maxread.substack.com/p/ai-as-normal-technology-derogatory">A.I. as normal technology (derogatory)</a> <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
  </div>
</article>
<script src="/assets/js/image-gallery.js"></script>
      </main>
    </div>

    <script src="/assets/js/main.js"></script>
  </body>
</html>
