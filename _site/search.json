[
  
  
    {
      "title": "What Are We Even Doing?",
      "date": "08 Oct 2025",
      "permalink": "/2025/10/08/what_are_we_even_doing.html",
      "content": "<p>Whilst browsing YouTube, I came across <a href=\"https://www.youtube.com/@AI_In_Context\">AI In Context</a>’s ‘<a href=\"https://www.youtube.com/watch?v=5KVDDfAkRgc\">We’re Not Ready for Superintelligence</a>’. This video, to me, was conspiratorial and made a lot of assumptions that just felt wrong. I mean just look at the description of the video:</p><blockquote>  <p>AI 2027 depicts a possible future where artificial intelligence radically transforms the world in just a few intense years. It’s based on detailed expert forecasts — but how much of it will actually happen? Are we really racing towards a choice between a planet controlled by the elite, or one where humans have lost control entirely?</p>  <p>My takeaway? Loss of control, racing scenarios, and concentration of power are all concerningly plausible, and among the most pressing issues the world faces. <sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\" rel=\"footnote\">1</a></sup></p></blockquote><iframe src=\"https://www.youtube.com/embed/5KVDDfAkRgc\" title=\"We&#39;re Not Ready for Superintelligence\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"></iframe><p>So after watching that conspiratorial apocalyptic mess, I decided to do some digging, and this is what I found.</p><p><a href=\"https://www.imdb.com/name/nm5354308/\">Aric Floyd</a>, the host of the channel is an actor. The channel is not a personal project of some AI enthusiast. It was created by the nonprofit ‘<a href=\"https://80000hours.org/\">80,000 Hours</a>’.</p><p>You might recognize 80,000 Hours from a YouTube sponsor read. Here’s one I found from Extra Credits:</p><blockquote>  <p>80,000 Hours is a nonprofit organization dedicated to helping people find careers that are not only fulfilling but also have a significant positive impact on the world. Their work is based on over 10 years of research, conducted in collaboration with Oxford University, aimed at identifying high-impact career paths that can tackle some of humanity’s biggest challenges.<sup id=\"fnref:2\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\" rel=\"footnote\">2</a></sup></p></blockquote><p>So you might be wondering as to why a career training website would be producing an apocalyptic AI video about how humanity will need to fight against an all-powerful AI. The answer is that 80,000 Hours aligns itself with the philosophy of effective altruism.</p><blockquote>  <p>Effective Altruism is a form of utilitarianism, itself a branch of consequentialism – the plausible idea that you judge choices by their consequences. Utilitarians go further by saying you judge acts only by their consequences, and that the outcomes can be graded on a single scale of utility, or happiness. You judge acts by comparing their effects on the global sum of happiness, and the morally right course is the one you expect to achieve the maximum effect. Crucially, other moral considerations – obligations to be honest, to be just, to be loyal, to respect property rights and many more – count only to the extent that they bear on the happiness calculation. <sup id=\"fnref:5\" role=\"doc-noteref\"><a href=\"#fn:5\" class=\"footnote\" rel=\"footnote\">3</a></sup></p></blockquote><p>I had first heard of effective altruism in 2022, as the philosophy of Sam Bankman-Fried. When FTX collapsed and Sam Bankman-Fried was sentenced on seven counts of wire fraud and conspiracy to launder money<sup id=\"fnref:3\" role=\"doc-noteref\"><a href=\"#fn:3\" class=\"footnote\" rel=\"footnote\">4</a></sup>, I thought that would be the end for this movement. I was wrong.</p><blockquote>  <p>The philosophy of effective altruism makes the case that one way that idealistic young people with fancy degrees could improve the world is not by working at nonprofits, but by earning as much money as possible through finance, and then donating it all to vetted causes. <sup id=\"fnref:4\" role=\"doc-noteref\"><a href=\"#fn:4\" class=\"footnote\" rel=\"footnote\">5</a></sup></p></blockquote><p>80,000 Hours advocates for a specific kind of effective altruism called “longtermism”<sup id=\"fnref:6\" role=\"doc-noteref\"><a href=\"#fn:6\" class=\"footnote\" rel=\"footnote\">6</a></sup>, the idea that “…because so many more humans and other intelligent beings could live in the future than live today, the most important thing for altruistic people to do in the present is to promote the welfare of those unborn beings, by ensuring that future comes to be by preventing existential risks — and that such a future is as good as possible.”<sup id=\"fnref:7\" role=\"doc-noteref\"><a href=\"#fn:7\" class=\"footnote\" rel=\"footnote\">7</a></sup></p><blockquote>  <p>For various reasons, the [Effective Altruism] movement has turned its attention toward longtermism—a more radical form of its utilitarianism that weighs the value of each future potential life approximately the same as a living person’s. Because any human extinction event, however unlikely, imposes infinite costs, longtermists can place enormous moral value on reducing whatever they view as existential risk. <sup id=\"fnref:9\" role=\"doc-noteref\"><a href=\"#fn:9\" class=\"footnote\" rel=\"footnote\">8</a></sup></p></blockquote><blockquote>  <p>[Effective Altruists] initially focused mostly on issues like animal welfare and global poverty, but over time, worries about an AI-fueled apocalypse became a central focus. With funding from deep-pocketed donors like billionaire and Facebook co-founder Dustin Moskovitz, they built their own insular universe to study AI safety, including a web of nonprofits and research organizations, forecasting centers, conferences, and web forums.<sup id=\"fnref:8\" role=\"doc-noteref\"><a href=\"#fn:8\" class=\"footnote\" rel=\"footnote\">9</a></sup></p></blockquote><blockquote>  <p>The most ardent advocates of effective altruism, or EA, believe researchers are only months or years away from building an AI superintelligence able to outsmart the world’s collective efforts to control it. Through either its own volition or via terrorists seeking to develop deadly bioweapons, such an AI could wipe out humanity, they say. And some, including noted EA thinker Eliezer Yudkowsky, believe even a nuclear holocaust would be preferable to an unchecked AI future. <sup id=\"fnref:10\" role=\"doc-noteref\"><a href=\"#fn:10\" class=\"footnote\" rel=\"footnote\">10</a></sup></p></blockquote><blockquote>  <p>Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that’s what it takes to reduce the risk of large AI training runs. <sup id=\"fnref:11\" role=\"doc-noteref\"><a href=\"#fn:11\" class=\"footnote\" rel=\"footnote\">11</a></sup></p></blockquote><p>This to me does not sound like philosophy; this sounds like an AI <a href=\"https://en.wikipedia.org/wiki/Doomsday_cult\">doomsday cult</a>.</p><blockquote>  <p>[Effective Altruism]’s career advice center, 80,000 hours, lists “AI safety technical research” and “shaping future governance of AI” as the top two recommended careers for EAs to go into, and the billionaire EA class funds initiatives attempting to stop an AGI apocalypse. According to EAs, AGI is likely inevitable, and their goal is thus to make it beneficial to humanity: akin to creating a benevolent god rather than a devil. <sup id=\"fnref:12\" role=\"doc-noteref\"><a href=\"#fn:12\" class=\"footnote\" rel=\"footnote\">12</a></sup></p></blockquote><p>To borrow a term from Lee Vinsel, this AI apocalypse is another form of criti-hype.<sup id=\"fnref:15\" role=\"doc-noteref\"><a href=\"#fn:15\" class=\"footnote\" rel=\"footnote\">13</a></sup> Basically, another <a href=\"https://www.scientificamerican.com/article/premature-freak-outs-about-techno-enhancement/\">Premature Freak-Out about Technological Enhancement</a>.</p><blockquote>  <p>Perhaps the two most striking examples of this criti-hype trend are Shoshana Zuboff’s book, The Age of Surveillance Capitalism, and the film The Social Dilemma, which includes Zuboff and another criti-hyper Tristan Harris as talking heads. <sup id=\"fnref:15:1\" role=\"doc-noteref\"><a href=\"#fn:15\" class=\"footnote\" rel=\"footnote\">13</a></sup></p></blockquote><p>Speaking of criti-hyper Tristan Harris<sup id=\"fnref:18\" role=\"doc-noteref\"><a href=\"#fn:18\" class=\"footnote\" rel=\"footnote\">14</a></sup>, I came across this interview he did with Jon Stewart on The Daily Show.</p><iframe src=\"https://www.youtube.com/embed/675d_6WGPbo\" title=\"Tristan Harris – The Dangers of Unregulated AI on Humanity &amp; the Workforce | The Daily Show\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"></iframe><p>Tristan Harris, after peddling the idea that social media companies are puppet masters who have users on strings<sup id=\"fnref:15:2\" role=\"doc-noteref\"><a href=\"#fn:15\" class=\"footnote\" rel=\"footnote\">13</a></sup>, has now moved onto his next target of AI, falling into the same industry propaganda that he did last time.</p><p>This is not to say that there are not any real reasons to be critical of AI. Back in 2020, Robert Julian-Borchak Williams was “wrongfully arrested based on a flawed match from a facial recognition algorithm”. <sup id=\"fnref:16\" role=\"doc-noteref\"><a href=\"#fn:16\" class=\"footnote\" rel=\"footnote\">15</a></sup> That same year, “two professors and a graduate student had developed a facial-recognition program that could predict whether someone would be a criminal”.<sup id=\"fnref:17\" role=\"doc-noteref\"><a href=\"#fn:17\" class=\"footnote\" rel=\"footnote\">16</a></sup></p><blockquote>  <p>Advances in data science and machine learning have led to numerous algorithms in recent years that purport to predict crimes or criminality. But if the data used to build those algorithms is biased, the algorithms’ predictions will also be biased. Because of the racially skewed nature of policing in the US, the letter argues, any predictive algorithm modeling criminality will only reproduce the biases already reflected in the criminal justice system. <sup id=\"fnref:17:1\" role=\"doc-noteref\"><a href=\"#fn:17\" class=\"footnote\" rel=\"footnote\">16</a></sup></p></blockquote><p>This is not even to mention the horrifying and harmful firehose of AI slop that is now being generated en masse. <sup id=\"fnref:19\" role=\"doc-noteref\"><a href=\"#fn:19\" class=\"footnote\" rel=\"footnote\">17</a></sup><sup id=\"fnref:20\" role=\"doc-noteref\"><a href=\"#fn:20\" class=\"footnote\" rel=\"footnote\">18</a></sup><sup id=\"fnref:21\" role=\"doc-noteref\"><a href=\"#fn:21\" class=\"footnote\" rel=\"footnote\">19</a></sup><sup id=\"fnref:22\" role=\"doc-noteref\"><a href=\"#fn:22\" class=\"footnote\" rel=\"footnote\">20</a></sup><sup id=\"fnref:23\" role=\"doc-noteref\"><a href=\"#fn:23\" class=\"footnote\" rel=\"footnote\">21</a></sup><sup id=\"fnref:24\" role=\"doc-noteref\"><a href=\"#fn:24\" class=\"footnote\" rel=\"footnote\">22</a></sup><sup id=\"fnref:25\" role=\"doc-noteref\"><a href=\"#fn:25\" class=\"footnote\" rel=\"footnote\">23</a></sup><sup id=\"fnref:26\" role=\"doc-noteref\"><a href=\"#fn:26\" class=\"footnote\" rel=\"footnote\">24</a></sup></p><p>I think that we need to treat AI as <a href=\"https://knightcolumbia.org/content/ai-as-normal-technology\">normal technology</a>.</p><blockquote>  <p>The statement “AI is normal technology” is three things: a description of current AI, a prediction about the foreseeable future of AI, and a prescription about how we should treat it. We view AI as a tool that we can and should remain in control of, and we argue that this goal does not require drastic policy interventions or technical breakthroughs. We do not think that viewing AI as a humanlike intelligence is currently accurate or useful for understanding its societal impacts, nor is it likely to be in our vision of the future. <sup id=\"fnref:13\" role=\"doc-noteref\"><a href=\"#fn:13\" class=\"footnote\" rel=\"footnote\">25</a></sup></p></blockquote><blockquote>  <p>That some significant portion of OpenAI’s consumer base is using ChatGPT not so much for the expected “normal” uses like search, or productivity improvements, or creating slop birthday-party invitations, but for friendship, companionship, romance, and therapy certainly feels abnormal. (And apocalyptic.) But this is 2025, and intense, emotional, addiction-resembling attachment to software-bound experience has been a core paradigm of the technology industry for almost two decades, not to mention a multibillion-dollar business model. Certainly, you will not find me arguing that “psychosis-inducing sycophantic girlfriend robot subscription product” is “normal” in the sense of “acceptable” or “appropriate to a mature and dignified civilization.” But speaking descriptively, as a matter of long precedent, what could be more normal, in Silicon Valley, than people weeping on a message board because a UX change has transformed the valence of their addiction? <sup id=\"fnref:14\" role=\"doc-noteref\"><a href=\"#fn:14\" class=\"footnote\" rel=\"footnote\">26</a></sup></p></blockquote><h2 id=\"further-reading\">Further Reading</h2><ul>  <li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a></li>  <li><a href=\"https://joinreboot.org/p/alignment\">The Artificiality of Alignment</a></li>  <li><a href=\"https://www.argmin.net/p/the-banal-evil-of-ai-safety\">The Banal Evil of AI Safety</a></li>  <li><a href=\"https://realizable.substack.com/p/artificial-intelligence-as-sorcery\">Artificial Intelligence as Sorcery</a></li>  <li><a href=\"https://aiguide.substack.com/p/magical-thinking-on-ai\">Magical Thinking on AI</a></li>  <li><a href=\"https://www.science.org/content/article/far-more-authors-use-ai-write-science-papers-admit-it-publisher-reports\">Far more authors use AI to write science papers than admit it, publisher reports</a></li></ul><h2 id=\"references\">References</h2><div class=\"footnotes\" role=\"doc-endnotes\">  <ol>    <li id=\"fn:1\" role=\"doc-endnote\">      <p><a href=\"https://www.youtube.com/watch?v=5KVDDfAkRgc\">We’re Not Ready for Superintelligence</a> <a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:2\" role=\"doc-endnote\">      <p><a href=\"https://www.youtube.com/watch?v=9PYewjJKD-4\">The Dev’s Creed: Being Wrong is Essential</a> <a href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:5\" role=\"doc-endnote\">      <p><a href=\"https://www.bloomberg.com/opinion/articles/2023-10-18/effective-altruism-is-as-bankrupt-as-samuel-bankman-fried-s-ftx\">Effective Altruism Is as Bankrupt as Sam Bankman-Fried’s FTX</a> <a href=\"#fnref:5\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:3\" role=\"doc-endnote\">      <p><a href=\"https://www.bbc.com/worklife/article/20231009-ftxs-sam-bankman-fried-believed-in-effective-altruism-what-is-it\">FTX’s Sam Bankman-Fried believed in ‘effective altruism’. What is it?</a> <a href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:4\" role=\"doc-endnote\">      <p><a href=\"https://fortune.com/2024/09/02/effective-altruism-sam-bankman-frieds-bahamas-penthouse-ftx-crypto-finance/\">Behind the scenes of how ‘effective altruism’ went to die in Sam Bankman-Fried’s Bahamas penthouse</a> <a href=\"#fnref:4\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:6\" role=\"doc-endnote\">      <p><a href=\"https://80000hours.org/articles/future-generations/\">Longtermism: a call to protect future generations</a> <a href=\"#fnref:6\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:7\" role=\"doc-endnote\">      <p><a href=\"https://www.vox.com/future-perfect/23500014/effective-altruism-sam-bankman-fried-ftx-crypto\">How effective altruism let Sam Bankman-Fried happen</a> <a href=\"#fnref:7\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:9\" role=\"doc-endnote\">      <p><a href=\"https://reason.com/2024/07/05/the-authoritarian-side-of-effective-altruism-comes-for-ai/\">The Authoritarian Side of Effective Altruism Comes for AI</a> <a href=\"#fnref:9\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:8\" role=\"doc-endnote\">      <p><a href=\"https://www.semafor.com/article/11/21/2023/how-effective-altruism-led-to-a-crisis-at-openai\">The AI industry turns against its favorite philosophy</a> <a href=\"#fnref:8\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:10\" role=\"doc-endnote\">      <p><a href=\"https://www.politico.com/news/2023/12/30/ai-debate-culture-clash-dc-silicon-valley-00133323\">When Silicon Valley’s AI warriors came to Washington</a> <a href=\"#fnref:10\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:11\" role=\"doc-endnote\">      <p><a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Pausing AI Developments Isn’t Enough. We Need to Shut it All Down</a> <a href=\"#fnref:11\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:12\" role=\"doc-endnote\">      <p><a href=\"https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/\">Effective Altruism Is Pushing a Dangerous Brand of ‘AI Safety’</a> <a href=\"#fnref:12\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:15\" role=\"doc-endnote\">      <p><a href=\"https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5\">You’re Doing It Wrong: Notes on Criticism and Technology Hype</a> <a href=\"#fnref:15\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:15:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a> <a href=\"#fnref:15:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>3</sup></a></p>    </li>    <li id=\"fn:18\" role=\"doc-endnote\">      <p><a href=\"https://80000hours.org/podcast/episodes/tristan-harris-changing-incentives-social-media/\">Tristan Harris on the need to change the incentives of social media companies</a> <a href=\"#fnref:18\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:16\" role=\"doc-endnote\">      <p><a href=\"https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html\">Wrongfully Accused by an Algorithm</a> <a href=\"#fnref:16\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:17\" role=\"doc-endnote\">      <p><a href=\"https://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/\">An Algorithm That ‘Predicts’ Criminality Based on a Face Sparks a Furor</a> <a href=\"#fnref:17\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:17:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>    </li>    <li id=\"fn:19\" role=\"doc-endnote\">      <p><a href=\"https://www.theguardian.com/technology/2024/mar/06/microsoft-ai-explicit-image-safety\">Microsoft ignored safety problems with AI image generator, engineer complains</a> <a href=\"#fnref:19\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:20\" role=\"doc-endnote\">      <p><a href=\"https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/\">This is how AI image generators see the world</a> <a href=\"#fnref:20\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:21\" role=\"doc-endnote\">      <p><a href=\"https://www.404media.co/4chan-uses-bing-to-flood-the-internet-with-racist-images/\">4chan Uses Bing to Flood the Internet With Racist Images</a> <a href=\"#fnref:21\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:22\" role=\"doc-endnote\">      <p><a href=\"https://www.404media.co/where-facebooks-ai-slop-comes-from/\">Where Facebook’s AI Slop Comes From</a> <a href=\"#fnref:22\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:23\" role=\"doc-endnote\">      <p><a href=\"https://www.washingtonpost.com/technology/2023/12/14/ai-hate-memes-antisemitic-musk-x/\">AI-generated Nazi memes thrive on Musk’s X despite claims of crackdown</a> <a href=\"#fnref:23\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:24\" role=\"doc-endnote\">      <p><a href=\"https://www.404media.co/brainrot-ai-on-instagram-is-monetizing-the-most-fucked-up-things-you-can-imagine-and-lots-you-cant/\">‘Brainrot’ AI on Instagram Is Monetizing the Most Fucked Up Things You Can Imagine (and Lots You Can’t)</a> <a href=\"#fnref:24\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:25\" role=\"doc-endnote\">      <p><a href=\"https://www.mediamatters.org/tiktok/racist-ai-generated-videos-are-newest-slop-garnering-millions-views-tiktok\">Racist AI-generated videos are the newest slop garnering millions of views on TikTok</a> <a href=\"#fnref:25\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:26\" role=\"doc-endnote\">      <p><a href=\"https://www.theguardian.com/technology/2025/jan/13/just-the-start-xs-new-ai-software-driving-online-racist-abuse-experts-warn\">‘Just the start’: X’s new AI software driving online racist abuse, experts warn</a> <a href=\"#fnref:26\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:13\" role=\"doc-endnote\">      <p><a href=\"https://knightcolumbia.org/content/ai-as-normal-technology\">AI as Normal Technology</a> <a href=\"#fnref:13\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:14\" role=\"doc-endnote\">      <p><a href=\"https://maxread.substack.com/p/ai-as-normal-technology-derogatory\">A.I. as normal technology (derogatory)</a> <a href=\"#fnref:14\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>  </ol></div>",
      "image": "/assets/images/what_are_we_even_doing/Screenshot%202025-10-08%20at%2018-05-46%20ChatGPT.png"
    },
  
    {
      "title": "Don't Be Evil",
      "date": "27 Sep 2025",
      "permalink": "/2025/09/27/dont_be_evil.html",
      "content": "<blockquote>  <p>We believe that Google should not be in the business of war. <sup id=\"fnref:3\" role=\"doc-noteref\"><a href=\"#fn:3\" class=\"footnote\" rel=\"footnote\">1</a></sup></p></blockquote><p>In 2018, over 4,000 Google employees signed a letter protesting the company’s involvement in the Pentagon’s Project Maven.<sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\" rel=\"footnote\">2</a></sup><sup id=\"fnref:2\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\" rel=\"footnote\">3</a></sup> Google’s involvement with this Project Maven had been concealed through a third-party contractor called ECS Federal. <sup id=\"fnref:10\" role=\"doc-noteref\"><a href=\"#fn:10\" class=\"footnote\" rel=\"footnote\">4</a></sup><sup id=\"fnref:11\" role=\"doc-noteref\"><a href=\"#fn:11\" class=\"footnote\" rel=\"footnote\">5</a></sup><sup id=\"fnref:12\" role=\"doc-noteref\"><a href=\"#fn:12\" class=\"footnote\" rel=\"footnote\">6</a></sup> Project Maven used artificial intelligence to interpret video imagery that could be used to improve the targeting of drone strikes.<sup id=\"fnref:1:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\" rel=\"footnote\">2</a></sup><sup id=\"fnref:2:1\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\" rel=\"footnote\">3</a></sup> Some employees resigned in protest, while others were openly advocating for the company to cancel the contract.<sup id=\"fnref:2:2\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\" rel=\"footnote\">3</a></sup></p><blockquote>  <p>Amid growing fears of biased and weaponized AI, Google is already struggling to keep the public’s trust. By entering into this contract, Google will join the ranks of companies like Palantir, Raytheon, and General Dynamics. The argument that other firms, like Microsoft and Amazon, are also participating doesn’t make this any less risky for Google. Google’s unique history, its motto <em>Don’t Be Evil</em>, and its direct reach into the lives of billions of users set it apart.<sup id=\"fnref:3:1\" role=\"doc-noteref\"><a href=\"#fn:3\" class=\"footnote\" rel=\"footnote\">1</a></sup></p></blockquote><p>In response to all of this, Google said it wouldn’t renew the contract<sup id=\"fnref:2:3\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\" rel=\"footnote\">3</a></sup> and Google CEO Sundar Pichai offered rules for the company’s use of artificial intelligence.<sup id=\"fnref:4\" role=\"doc-noteref\"><a href=\"#fn:4\" class=\"footnote\" rel=\"footnote\">7</a></sup> The guiding principles forbade work on weapons and surveillance projects “violating internationally accepted norms.”<sup id=\"fnref:5\" role=\"doc-noteref\"><a href=\"#fn:5\" class=\"footnote\" rel=\"footnote\">8</a></sup></p><p>Around the same time as the Project Maven protests, Google removed the “Don’t Be Evil” clause from its code of conduct. <sup id=\"fnref:14\" role=\"doc-noteref\"><a href=\"#fn:14\" class=\"footnote\" rel=\"footnote\">9</a></sup></p><blockquote>  <p>“Don’t be evil” has been part of the company’s corporate code of conduct since 2000. When Google was reorganized under a new parent company, Alphabet, in 2015, Alphabet assumed a slightly adjusted version of the motto, “do the right thing.” However, Google retained its original “don’t be evil” language until the past several weeks.<sup id=\"fnref:14:1\" role=\"doc-noteref\"><a href=\"#fn:14\" class=\"footnote\" rel=\"footnote\">9</a></sup></p></blockquote><p>In a 2013 NPR interview, Eric Schmidt, Google’s executive chairman at the time, stated that he thought <em>Don’t be evil</em> “was the stupidest rule ever, because there’s no book about evil except maybe, you know, the Bible or something.”<sup id=\"fnref:15\" role=\"doc-noteref\"><a href=\"#fn:15\" class=\"footnote\" rel=\"footnote\">10</a></sup></p><p>In 2016, Eric Schmidt, now Alphabet’s executive chair, became chair of the Defense Innovation Advisory Board.<sup id=\"fnref:5:1\" role=\"doc-noteref\"><a href=\"#fn:5\" class=\"footnote\" rel=\"footnote\">8</a></sup> Google won the Project Maven contract in late 2017.<sup id=\"fnref:5:2\" role=\"doc-noteref\"><a href=\"#fn:5\" class=\"footnote\" rel=\"footnote\">8</a></sup></p><blockquote>  <p>SCHMIDT: So what happens is, I’m sitting in this meeting, and we’re having this debate about an advertising product. And one of the engineers pounds his fists on the table and says, that’s evil. And then the whole conversation stops, everyone goes into conniptions, and eventually we stopped the project. So it did work.<sup id=\"fnref:15:1\" role=\"doc-noteref\"><a href=\"#fn:15\" class=\"footnote\" rel=\"footnote\">10</a></sup></p></blockquote><p>In 2021, over 1,100 Google and Amazon employees wrote an open letter demanding that Google drop Project Nimbus.<sup id=\"fnref:8\" role=\"doc-noteref\"><a href=\"#fn:8\" class=\"footnote\" rel=\"footnote\">11</a></sup> Project Nimbus is a $1.2bn contract to provide cloud services for the Israeli military and government. <sup id=\"fnref:6\" role=\"doc-noteref\"><a href=\"#fn:6\" class=\"footnote\" rel=\"footnote\">12</a></sup> In 2022, hundreds of Google and Amazon employees and their supporters protested outside of company offices in San Francisco, New York, Seattle, and Durham, N.C., demanding that the tech giants end their Project Nimbus contracts.<sup id=\"fnref:8:1\" role=\"doc-noteref\"><a href=\"#fn:8\" class=\"footnote\" rel=\"footnote\">11</a></sup><sup id=\"fnref:9\" role=\"doc-noteref\"><a href=\"#fn:9\" class=\"footnote\" rel=\"footnote\">13</a></sup></p><p>And again in 2023, Google employees published another open letter, this time demanding that “Google stop providing material support to this genocide by canceling its Project Nimbus contract and immediately cease doing business with the Israeli apartheid government and military.”<sup id=\"fnref:7\" role=\"doc-noteref\"><a href=\"#fn:7\" class=\"footnote\" rel=\"footnote\">14</a></sup></p><blockquote>  <p>This is genocide. By supplying artificial intelligence and other technology to Israel, Google is complicit in the mass surveillance that enables the occupation and subjugation of Palestinians, the root cause of the unfolding violence.<sup id=\"fnref:8:2\" role=\"doc-noteref\"><a href=\"#fn:8\" class=\"footnote\" rel=\"footnote\">11</a></sup></p></blockquote><p>Workers looking to express support for Palestinians said that they faced hostility.<sup id=\"fnref:13\" role=\"doc-noteref\"><a href=\"#fn:13\" class=\"footnote\" rel=\"footnote\">15</a></sup></p><blockquote>  <p>Google managers have called employees “sick” and a “lost cause” for showing empathy towards the besieged residents of Gaza, and have even publicly asked Arab and Muslim Googlers if they support Hamas as a response to those Googlers expressing empathy or concern for Palestinian families’ safety.<sup id=\"fnref:7:1\" role=\"doc-noteref\"><a href=\"#fn:7\" class=\"footnote\" rel=\"footnote\">14</a></sup></p></blockquote><p>Google employees were able to successfully dissuade the company from Project Maven. But it seems very unlikely that they will be able to achieve the same results with Project Nimbus.<sup id=\"fnref:5:3\" role=\"doc-noteref\"><a href=\"#fn:5\" class=\"footnote\" rel=\"footnote\">8</a></sup></p><blockquote>  <p>It would be challenging for any employee protest to achieve the scale or impact of the Maven protests. Google’s legendarily freewheeling internal culture has become more locked-down since then. <sup id=\"fnref:5:4\" role=\"doc-noteref\"><a href=\"#fn:5\" class=\"footnote\" rel=\"footnote\">8</a></sup></p></blockquote><h2 id=\"references\">References</h2><div class=\"footnotes\" role=\"doc-endnotes\">  <ol>    <li id=\"fn:3\" role=\"doc-endnote\">      <p><a href=\"https://static01.nyt.com/files/2018/technology/googleletter.pdf\">Google Employees’ Letter to Sundar Pichai Regarding Project Maven</a> <a href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:3:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>    </li>    <li id=\"fn:1\" role=\"doc-endnote\">      <p><a href=\"https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-pentagon-project.html\">‘The Business of War’: Google Employees Protest Work for the Pentagon</a> <a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:1:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>    </li>    <li id=\"fn:2\" role=\"doc-endnote\">      <p><a href=\"https://www.nytimes.com/2018/06/01/technology/google-pentagon-project-maven.html\">Google Will Not Renew Pentagon Contract That Upset Employees</a> <a href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:2:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a> <a href=\"#fnref:2:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>3</sup></a> <a href=\"#fnref:2:3\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>4</sup></a></p>    </li>    <li id=\"fn:10\" role=\"doc-endnote\">      <p><a href=\"https://gizmodo.com/google-is-helping-the-pentagon-build-ai-for-drones-1823464533\">Google Is Helping the Pentagon Build AI for Drones</a> <a href=\"#fnref:10\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:11\" role=\"doc-endnote\">      <p><a href=\"https://theintercept.com/2018/03/06/google-is-quietly-providing-ai-technology-for-drone-strike-targeting-project/\">Google Is Quietly Providing AI Technology for Drone Strike Targeting Project</a> <a href=\"#fnref:11\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:12\" role=\"doc-endnote\">      <p><a href=\"https://theintercept.com/2020/10/21/google-cbp-border-contract-anduril/\">Google AI Tech Will Be Used for Virtual Border Wall, CBP Contract Shows</a> <a href=\"#fnref:12\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:4\" role=\"doc-endnote\">      <p><a href=\"https://www.wired.com/story/google-sets-limits-on-its-use-of-ai-but-allows-defense-work/\">Google Sets Limits on Its Use of AI but Allows Defense Work</a> <a href=\"#fnref:4\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:5\" role=\"doc-endnote\">      <p><a href=\"https://www.wired.com/story/3-years-maven-uproar-google-warms-pentagon/\">3 Years After the Project Maven Uproar, Google Cozies to the Pentagon</a> <a href=\"#fnref:5\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:5:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a> <a href=\"#fnref:5:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>3</sup></a> <a href=\"#fnref:5:3\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>4</sup></a> <a href=\"#fnref:5:4\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>5</sup></a></p>    </li>    <li id=\"fn:14\" role=\"doc-endnote\">      <p><a href=\"https://web.archive.org/web/20250831160020/https://gizmodo.com/google-removes-nearly-all-mentions-of-dont-be-evil-from-1826153393\">Google Removes ‘Don’t Be Evil’ Clause From Its Code of Conduct</a> <a href=\"#fnref:14\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:14:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>    </li>    <li id=\"fn:15\" role=\"doc-endnote\">      <p><a href=\"https://www.npr.org/2013/05/11/182873683/google-chairman-eric-schmidt-plays-not-my-job\">Google Chairman Eric Schmidt Plays Not My Job</a> <a href=\"#fnref:15\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:15:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>    </li>    <li id=\"fn:8\" role=\"doc-endnote\">      <p><a href=\"https://docs.google.com/document/u/0/d/e/2PACX-1vQt-eWcx-7rZxWTlx0dngRvhn_goqMdl8bPhqvucPiEenbd6KNpLGe-I_QLPLg1_K37Yrkp86ks4RXl/pub?pli=1\">Workers Call on Google Leadership to Drop Nimbus and Publicly Condemn Genocide in Gaza</a> <a href=\"#fnref:8\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:8:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a> <a href=\"#fnref:8:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>3</sup></a></p>    </li>    <li id=\"fn:6\" role=\"doc-endnote\">      <p><a href=\"https://www.theguardian.com/commentisfree/2021/oct/12/google-amazon-workers-condemn-project-nimbus-israeli-military-contract\">We are Google and Amazon workers. We condemn Project Nimbus</a> <a href=\"#fnref:6\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:9\" role=\"doc-endnote\">      <p><a href=\"https://www.sfchronicle.com/tech/article/Google-Amazon-employees-protest-tech-giants-17428743.php\">Google, Amazon employees protest tech giants’ contract with Israel as worker activism ramps up</a> <a href=\"#fnref:9\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>    <li id=\"fn:7\" role=\"doc-endnote\">      <p><a href=\"https://medium.com/@notechforapartheid/googleopenletter-868f0c4477db\">Open Letter: Google Workers Condemn Internal Culture of Hate, Abuse, and Retaliation</a> <a href=\"#fnref:7\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:7:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>    </li>    <li id=\"fn:13\" role=\"doc-endnote\">      <p><a href=\"https://www.nytimes.com/2023/11/08/business/israel-palestine-google-employees.html\">Google’s Open Culture Collides With the Israel-Hamas War</a> <a href=\"#fnref:13\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>    </li>  </ol></div>",
      "image": "/assets/images/dont_be_evil/Screenshot%202025-09-27%20at%2008-18-20%20Google.png"
    },
  
    {
      "title": "GLB File Format",
      "date": "04 Jan 2025",
      "permalink": "/2025/01/04/glb_file_format.html",
      "content": "<p>I wanted to look at the quality of Google’s first page of search results, using search operators to exclude AI-written articles and remove Google’s AI Summary. I used the search operators <code class=\"language-plaintext highlighter-rouge\">before:</code> and <code class=\"language-plaintext highlighter-rouge\">-ai</code>.</p><p>ChatGPT launched in 2022, so by using <code class=\"language-plaintext highlighter-rouge\">before:</code> we can filter out results published before then. This will exclude newer articles that might be AI-written. To remove the AI Overview that now appears at the top of search results, we use <code class=\"language-plaintext highlighter-rouge\">-ai</code>.</p><p>The idea came from this video:</p><iframe src=\"https://www.youtube.com/embed/-opBifFfsMY\" title=\"Generative AI is a Parasitic Cancer\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"></iframe><hr /><p>Going through and using the search operators, we can see that the link for <a href=\"https://www.khronos.org/Gltf\">https://www.khronos.org/Gltf</a>, which is the organization behind glTF, isn’t on the first page of Google results. It only appears after using the <code class=\"language-plaintext highlighter-rouge\">-ai</code> search operator to remove the AI Summary.</p><p><img src=\"/assets/images/glb_file_format/glTF Overview - The Khronos Group Inc.png\" alt=\"Screenshots of Google's search results for 'glb file format' with different search operators, highlighting the links to 'glTF Overview - The Khronos Group Inc'\" /></p><p>If we highlight any links from khronos.org, we get links to <a href=\"https://www.khronos.org/files/gltf20-reference-guide.pdf\">https://www.khronos.org/files/gltf20-reference-guide.pdf</a> for our <code class=\"language-plaintext highlighter-rouge\">before:2018 -ai</code> search, and links to <a href=\"https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html\">https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html</a> for our <code class=\"language-plaintext highlighter-rouge\">before:2022 -ai</code> and <code class=\"language-plaintext highlighter-rouge\">before:2023 -ai</code> searches.</p><p><img src=\"/assets/images/glb_file_format/The Khronos Group Inc additional.png\" alt=\"Screenshots of Google's search results for 'glb file format' with different search operators, highlighting any links to khronos.org\" /></p><p>The featured snippet also doesn’t use the direct source <a href=\"https://www.khronos.org/glTF\">https://www.khronos.org/glTF</a>, unless we search <code class=\"language-plaintext highlighter-rouge\">before:2018 -ai</code> or earlier years.</p><p><img src=\"/assets/images/glb_file_format/featured snippet.png\" alt=\"Screenshots of Google's search results for 'glb file format' with different search operators, highlighting the featured snippets; khronos.org in blue, other sites in red\" /></p><p>ChatGPT-written articles have pushed out the actually useful results, and now Google is using those poorly written AI articles as sources for its AI Overview.</p><p><img src=\"/assets/images/glb_file_format/Screenshot 2025-01-04 065807.png\" alt=\"Screenshot of Google's AI Summary from a Google search of 'glb file format'\" /></p><hr /><p>Anyway, here’s some information about the “GLB file format.”</p><ul>  <li><code class=\"language-plaintext highlighter-rouge\">.glb</code> is the filename extension for Binary glTF (<a href=\"https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html#glb-file-format-specification\">1</a>).</li>  <li>glTF stands for Graphics Language Transmission Format or GL Transmission Format (<a href=\"https://www.loc.gov/preservation/digital/formats/fdd/fdd000498.shtml\">2</a>).</li>  <li>Binary glTF is a binary option for storing the content of a glTF asset (<a href=\"https://www.loc.gov/preservation/digital/formats/fdd/fdd000498.shtml\">2</a>).</li>  <li>A glTF asset is represented by a JSON-formatted file <code class=\"language-plaintext highlighter-rouge\">.gltf</code>, binary files <code class=\"language-plaintext highlighter-rouge\">.bin</code>, and image files <code class=\"language-plaintext highlighter-rouge\">.jpg</code>, <code class=\"language-plaintext highlighter-rouge\">.png</code> (<a href=\"https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html#gltf-basics\">3</a>).</li>  <li>glTF files are stored in a GLB container <code class=\"language-plaintext highlighter-rouge\">.glb</code> (<a href=\"https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html#file-extensions-and-media-types\">4</a>).</li>  <li>Link to the IANA registration: <a href=\"https://www.iana.org/assignments/media-types/model/gltf-binary\">https://www.iana.org/assignments/media-types/model/gltf-binary</a></li>  <li>Link to the Specification by the Khronos Group: <a href=\"https://www.khronos.org/gltf\">https://www.khronos.org/gltf</a></li>  <li>Link to the GLB File Format Specification: <a href=\"https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html#glb-file-format-specification\">https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html#glb-file-format-specification</a></li></ul><hr />",
      "image": "/assets/images/glb_file_format/Screenshot%202025-01-04%20065807.png"
    }
  
]
